{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_fist = \"fist/fist\"\n",
    "img_path_hand = \"hand/hand\"\n",
    "img_path_one = \"one/one\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "im = image.load_img(path=\"fist/fist1.png\",grayscale=True,target_size=(150,150,1))\n",
    "im = image.img_to_array(im)\n",
    "# img = image.load_img(path=\"3.jpg\",grayscale=True,target_size=(28,28,1))\n",
    "# img = image.img_to_array(img)\n",
    "a = np.array([im])\n",
    "#b = np.array([img])\n",
    "#c = np.concatenate((a, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,1000):\n",
    "    im = image.load_img(path=img_path_fist + str(i) + \".png\", grayscale = True, target_size=(150,150,1))\n",
    "    im = image.img_to_array(im)\n",
    "    b = np.array([im])\n",
    "    a = np.concatenate((a,b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,1000):\n",
    "    im = image.load_img(path=img_path_hand + str(i) + \".png\", grayscale = True, target_size=(150,150,1))\n",
    "    im = image.img_to_array(im)\n",
    "    b = np.array([im])\n",
    "    a = np.concatenate((a,b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py:492: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,1000):\n",
    "    im = image.load_img(path=img_path_one + str(i) + \".png\", grayscale = True, target_size=(150,150,1))\n",
    "    im = image.img_to_array(im)\n",
    "    b = np.array([im])\n",
    "    a = np.concatenate((a,b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2997, 150, 150, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('file2.txt', 'w+')\n",
    "z = np.array([1 for i in range(1,1000)], np.int32)\n",
    "b = np.array([2 for i in range(1,1000)], np.int32)\n",
    "c = np.array([3 for i in range(1,1000)], np.int32)\n",
    "for i in range(len(z)):\n",
    "    f.write(\"%i\\n\" % (z[i]))\n",
    "for i in range(len(b)):\n",
    "    f.write(\"%i\\n\" % (b[i]))\n",
    "for i in range(len(c)):\n",
    "    f.write(\"%i\\n\" % (c[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2997,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('file2.txt', delimiter=\",\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  3\n",
      "Output classes :  [1. 2. 3.]\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(data)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)\n",
    "print(len(a[0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2997, 150, 150, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.reshape(-1, 150,150, 1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.astype('float32')\n",
    "a = a / 255.\n",
    "data_one_hot = to_categorical(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 1.0\n",
      "After conversion to one-hot: [0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Original label:', data[256])\n",
    "print('After conversion to one-hot:', data_one_hot[256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,valid_X,train_label,valid_label = train_test_split(a, data_one_hot, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2397, 150, 150, 1), (600, 150, 150, 1), (2397, 4), (600, 4))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 15\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(150,150,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 32)      320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 150, 150, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 38, 38, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 38, 38, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               5914752   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 6,007,940\n",
      "Trainable params: 6,007,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_train = fashion_model.fit(train_X, train_label, \n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=epochs,\n",
    "                                  verbose=1,\n",
    "                                  validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2397 samples, validate on 600 samples\n",
      "Epoch 1/15\n",
      "2397/2397 [==============================] - 71s 30ms/step - loss: 0.9200 - acc: 0.5949 - val_loss: 0.2291 - val_acc: 0.9200\n",
      "Epoch 2/15\n",
      "2397/2397 [==============================] - 70s 29ms/step - loss: 0.0719 - acc: 0.9804 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "Epoch 3/15\n",
      "2397/2397 [==============================] - 69s 29ms/step - loss: 0.0241 - acc: 0.9925 - val_loss: 0.0233 - val_acc: 0.9917\n",
      "Epoch 4/15\n",
      "2397/2397 [==============================] - 70s 29ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 3.7369e-04 - val_acc: 1.0000\n",
      "Epoch 5/15\n",
      "2397/2397 [==============================] - 69s 29ms/step - loss: 4.1232e-04 - acc: 1.0000 - val_loss: 3.7780e-04 - val_acc: 1.0000\n",
      "Epoch 6/15\n",
      "2397/2397 [==============================] - 70s 29ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0036 - val_acc: 0.9983\n",
      "Epoch 7/15\n",
      "2397/2397 [==============================] - 70s 29ms/step - loss: 0.0024 - acc: 0.9996 - val_loss: 2.2797e-04 - val_acc: 1.0000\n",
      "Epoch 8/15\n",
      "2397/2397 [==============================] - 70s 29ms/step - loss: 6.4387e-04 - acc: 1.0000 - val_loss: 2.2361e-04 - val_acc: 1.0000\n",
      "Epoch 9/15\n",
      "2397/2397 [==============================] - 1559s 650ms/step - loss: 1.9786e-04 - acc: 1.0000 - val_loss: 1.7793e-04 - val_acc: 1.0000\n",
      "Epoch 10/15\n",
      "2397/2397 [==============================] - 69s 29ms/step - loss: 1.1433e-04 - acc: 1.0000 - val_loss: 1.0232e-04 - val_acc: 1.0000\n",
      "Epoch 11/15\n",
      "2397/2397 [==============================] - 69s 29ms/step - loss: 5.9448e-05 - acc: 1.0000 - val_loss: 7.6760e-05 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      "2397/2397 [==============================] - 69s 29ms/step - loss: 3.8300e-05 - acc: 1.0000 - val_loss: 6.4915e-05 - val_acc: 1.0000\n",
      "Epoch 13/15\n",
      "2397/2397 [==============================] - 69s 29ms/step - loss: 3.2031e-05 - acc: 1.0000 - val_loss: 5.6126e-05 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      "2397/2397 [==============================] - 69s 29ms/step - loss: 2.6603e-05 - acc: 1.0000 - val_loss: 4.8399e-05 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      "2397/2397 [==============================] - 69s 29ms/step - loss: 2.3620e-05 - acc: 1.0000 - val_loss: 4.2642e-05 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fe2d7be673b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#import time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#time.sleep(5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "#import cv2\n",
    "#import time\n",
    "#time.sleep(5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "i = 0\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    th1 = cv2.resize(gray, (150, 150))\n",
    "    th1 = image.img_to_array(th1)\n",
    "    npa = np.array([th1])\n",
    "    predicted_classes = fashion_model.predict(npa)\n",
    "    #ret,th1 = cv2.threshold(gray,80,255,cv2.THRESH_BINARY)\n",
    "    #cv2.imwrite('TEST' + str(i) + '.png',gray)\n",
    "    # Display the resulting frame\n",
    "    #cv2.imshow('frame',gray)\n",
    "    #print(predicted_classes)\n",
    "    fist = np.array([0,1,0,0])\n",
    "    hand = np.array([0,0,1,0])\n",
    "    one = np.array([0,0,0,1])\n",
    "    #print(predicted_classes)\n",
    "    cl = \"none\"\n",
    "    #print((predicted_classes == fist).all())\n",
    "    if ((predicted_classes == fist).all()):\n",
    "        cl = \"fist\"\n",
    "    if ((predicted_classes == hand).all()):\n",
    "        cl = \"hand\"\n",
    "    if ((predicted_classes == one).all()):\n",
    "        cl = \"one\"\n",
    "    cv2.putText(gray,cl,(0,130), 20, 1, (200,255,155), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    #i += 1\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
